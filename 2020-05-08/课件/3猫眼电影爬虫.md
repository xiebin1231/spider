# 猫眼电影爬虫

## 1. 本节目标

本节中，我们要提取出猫眼电影TOP100的电影名称、时间、评分、图片等信息，提取的站点URL为http://maoyan.com/board/4，提取的结果会以文件形式保存下来。

## 2. 抓取分析

我们需要抓取的目标站点为 http://maoyan.com/board/4，打开之后便可以查看到榜单信息

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200505113359775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3MzA0Mw==,size_16,color_FFFFFF,t_70)

排名第一的电影是霸王别姬，页面中显示的有效信息有影片名称、主演、上映时间、上映地区、评分、图片等信息。

将网页滚动到最下方，可以发现有分页的列表，直接点击第 2 页，观察页面的 URL 和内容发生了怎样的变化，如图  所示。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200505113436327.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3MzA0Mw==,size_16,color_FFFFFF,t_70)

可以发现页面的 URL 变成 http://maoyan.com/board/4?offset=10，比之前的 URL 多了一个参数，那就是 offset=10，而目前显示的结果是排行 11~20 名的电影，初步推断这是一个偏移量的参数。再点击下一页，发现页面的 URL 变成了 http://maoyan.com/board/4?offset=20，参数 offset 变成了 20，而显示的结果是排行 21~30 的电影。

由此可以总结出规律，offset 代表偏移量值，如果偏移量为 n，则显示的电影序号就是 n+1 到 n+10，每页显示 10 个。所以，如果想获取 TOP100 电影，只需要分开请求 10 次，而 10 次的 offset 参数分别设置为 0、10、20…90 即可，这样获取不同的页面之后，再用xpath提取出相关信息，就可以得到 TOP100 的所有电影信息了。

## 3 .抓取首页

接下来用代码实现这个过程。首先抓取第一页的内容。我们实现了 get_one_page 方法，并给它传入 url 参数。然后将抓取的页面结果返回，再通过 main 方法调用。初步代码实现如下：

```python
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
}


def get_one_page(url):
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.content.decode()
    return None


def main():
    url = 'http://maoyan.com/board/4'
    html = get_one_page(url)
    print(html)


main()
```

这样运行之后，就可以成功获取首页的源代码了。获取源代码后，就需要解析页面，提取出我们想要的信息。

## 4. 解析网页数据

```python
def parse_one_page(text):
    html = etree.HTML(text)
    # 获取10条电影信息节点
    films_infos = html.xpath('//div[@class="board-item-content"]')
    for films_info in films_infos:
        # 电影名
        name = films_info.xpath('.//p[@class="name"]/a/text()')
        name = name[0].strip() if name else None
        
        # 主演
        stars = films_info.xpath('.//p[@class="star"]/text()')
        stars = stars[0].strip() if stars else None
        
        # 上映时间
        releasetime = films_info.xpath('.//p[@class="releasetime"]/text()')
        releasetime = releasetime[0].strip() if releasetime else None
        
        # 评分
        score = films_info.xpath('.//p[@class="score"]/i/text()')
        score = ''.join(score)
        print(name, stars, releasetime, score)
```

## 5. 保存数据

```python
def save_data(data):
    with open('film.txt', 'a') as F:
        F.write(data + '\n')
```

## 6. 整体调度

```python
import requests
from lxml import etree

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
}


def get_one_page(url):
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.content.decode()
    return None


def parse_one_page(text):
    html = etree.HTML(text)
    # 获取10条电影信息节点
    films_infos = html.xpath('//div[@class="board-item-content"]')
    for films_info in films_infos:
        # 电影名
        name = films_info.xpath('.//p[@class="name"]/a/text()')
        name = name[0].strip() if name else None

        # 主演
        stars = films_info.xpath('.//p[@class="star"]/text()')
        stars = stars[0].strip() if stars else None

        # 上映时间
        releasetime = films_info.xpath('.//p[@class="releasetime"]/text()')
        releasetime = releasetime[0].strip() if releasetime else None

        # 评分
        score = films_info.xpath('.//p[@class="score"]/i/text()')
        score = ''.join(score)
        film_info = '\t'.join([name, stars, releasetime, score])
        yield film_info


def save_data(data):
    with open('film.txt', 'a') as F:
        F.write(data + '\n')


def main():
    url = 'https://maoyan.com/board/4?offset={}'
    for i in range(0,91,10):
        html = get_one_page(url.format(i))
        for line in parse_one_page(html):
            save_data(line)


main()

```

